{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/KGQYOAxDu+EL58RwThRr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Varshitha-bit/nlp/blob/main/Lab8_NGram_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step-2** Import required libraries\n",
        "\n",
        "• text preprocessing\n",
        "\n",
        "• tokenization\n",
        "\n",
        "• counting N-grams\n",
        "\n",
        "• probability calculations"
      ],
      "metadata": {
        "id": "1mUZbJuHOADZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgCxInsYN-nH",
        "outputId": "8a809040-90c8-4440-aacf-15bce78482bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (6.0.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n"
          ]
        }
      ],
      "source": [
        "# Text preprocessing\n",
        "import re\n",
        "import string\n",
        "\n",
        "# Tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# N-gram generation and counting\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "\n",
        "# Probability calculations\n",
        "import math\n",
        "!pip install python-docx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 3**\n",
        "\n",
        "Load dataset\n",
        "\n",
        "• load or copy text corpus\n",
        "\n",
        "• clean unnecessary lines\n",
        "\n",
        "• display sample text\n",
        "\n",
        "• explain dataset in 5–6 lines"
      ],
      "metadata": {
        "id": "3So-XpWVPLar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "import re\n",
        "\n",
        "# Load the Word document\n",
        "try:\n",
        "    doc = Document(\"data_set-8.docx\")\n",
        "\n",
        "    # Extract text from all paragraphs\n",
        "    text = \"\"\n",
        "    for para in doc.paragraphs:\n",
        "        text += para.text + \" \"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove numbers and special characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    print(\"Sample Text:\")\n",
        "    print(text[:500])   # Display first 500 characters\n",
        "except Exception as e:\n",
        "    print(f\"Error loading document: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvxPG3CbQjum",
        "outputId": "29defde6-685c-4d98-c31d-abaae6b06e18"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Text:\n",
            "natural language processing is a field of artificial intelligence it helps computers understand human language students use nlp techniques for text analysis language models predict the next word in a sentence n gram models are widely used in nlp probability plays an important role in language modeling text preprocessing improves the quality of data tokenization breaks text into meaningful units nlp is used in chatbots and search engines learning nlp is important for modern applications\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is a manually created text corpus containing simple and meaningful English sentences.\n",
        "It focuses on concepts related to natural language processing and general computing.\n",
        "The corpus is stored in plain text format, making it easy to load and process.\n",
        "Unnecessary symbols and formatting are minimal to simplify preprocessing.\n",
        "This dataset is suitable for performing tokenization, N-gram generation, and probability calculations.\n",
        "It helps demonstrate the fundamental concepts of language modeling in NLP."
      ],
      "metadata": {
        "id": "law4E-qqSTr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 4**\n",
        "\n",
        "Preprocess Text\n",
        "Write functions to:\n",
        "\n",
        "• convert to lowercase\n",
        "\n",
        "• remove punctuation and numbers\n",
        "\n",
        "• tokenize words\n",
        "\n",
        "• optionally remove stopwords\n",
        "\n",
        "• add start/end tokens for sentences (e.g., <s>, </s>)\n",
        "\n",
        "Students must briefly explain each step."
      ],
      "metadata": {
        "id": "UBESgxx_SlxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4 — Preprocess Text\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Convert text to lowercase\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Remove punctuation and numbers\n",
        "def remove_punctuation_numbers(text):\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Tokenize words\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Remove stopwords (optional)\n",
        "def remove_stopwords(tokens):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "# Add start and end tokens\n",
        "def add_start_end_tokens(tokens):\n",
        "    return ['<s>'] + tokens + ['</s>']\n",
        "\n",
        "\n",
        "# Apply preprocessing steps\n",
        "# Note: 'text' variable comes from the previous cell\n",
        "processed_text = to_lowercase(text)\n",
        "processed_text = remove_punctuation_numbers(processed_text)\n",
        "tokens = tokenize_text(processed_text)\n",
        "tokens = remove_stopwords(tokens)   # Optional\n",
        "tokens = add_start_end_tokens(tokens)\n",
        "\n",
        "# Display sample output\n",
        "print(\"Sample Preprocessed Tokens:\")\n",
        "print(tokens[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP9BE3_AS_ur",
        "outputId": "e7aa91b2-3147-4d77-f4b4-6c428fd1e559"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Preprocessed Tokens:\n",
            "['<s>', 'natural', 'language', 'processing', 'field', 'artificial', 'intelligence', 'helps', 'computers', 'understand', 'human', 'language', 'students', 'use', 'nlp', 'techniques', 'text', 'analysis', 'language', 'models', 'predict', 'next', 'word', 'sentence', 'n', 'gram', 'models', 'widely', 'used', 'nlp']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5 — Build N-Gram Models\n",
        "\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# ---------- UNIGRAM MODEL ----------\n",
        "\n",
        "# Count unigrams\n",
        "unigram_counts = Counter(tokens)\n",
        "total_tokens = sum(unigram_counts.values())\n",
        "\n",
        "# Calculate unigram probabilities\n",
        "unigram_prob = {\n",
        "    word: count / total_tokens\n",
        "    for word, count in unigram_counts.items()\n",
        "}\n",
        "\n",
        "# Create unigram table\n",
        "unigram_table = pd.DataFrame({\n",
        "    'Word': unigram_counts.keys(),\n",
        "    'Count': unigram_counts.values(),\n",
        "    'Probability': [unigram_prob[word] for word in unigram_counts.keys()]\n",
        "})\n",
        "\n",
        "print(\"Unigram Model\")\n",
        "print(unigram_table.head())\n",
        "\n",
        "\n",
        "# ---------- BIGRAM MODEL ----------\n",
        "\n",
        "# Generate and count bigrams\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "bigram_counts = Counter(bigrams)\n",
        "\n",
        "# Calculate bigram conditional probabilities\n",
        "# P(w2 | w1) = count(w1, w2) / count(w1)\n",
        "bigram_prob = {\n",
        "    bigram: count / unigram_counts[bigram[0]]\n",
        "    for bigram, count in bigram_counts.items()\n",
        "}\n",
        "\n",
        "# Create bigram table\n",
        "bigram_table = pd.DataFrame({\n",
        "    'Bigram': bigram_counts.keys(),\n",
        "    'Count': bigram_counts.values(),\n",
        "    'Conditional Probability': [bigram_prob[bg] for bg in bigram_counts.keys()]\n",
        "})\n",
        "\n",
        "print(\"\\nBigram Model\")\n",
        "print(bigram_table.head())\n",
        "\n",
        "\n",
        "# ---------- TRIGRAM MODEL ----------\n",
        "\n",
        "# Generate and count trigrams\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "trigram_counts = Counter(trigrams)\n",
        "\n",
        "# Calculate trigram conditional probabilities\n",
        "# P(w3 | w1, w2) = count(w1, w2, w3) / count(w1, w2)\n",
        "trigram_prob = {\n",
        "    trigram: count / bigram_counts[(trigram[0], trigram[1])]\n",
        "    for trigram, count in trigram_counts.items()\n",
        "}\n",
        "\n",
        "# Create trigram table\n",
        "trigram_table = pd.DataFrame({\n",
        "    'Trigram': trigram_counts.keys(),\n",
        "    'Count': trigram_counts.values(),\n",
        "    'Conditional Probability': [trigram_prob[tg] for tg in trigram_counts.keys()]\n",
        "})\n",
        "\n",
        "print(\"\\nTrigram Model\")\n",
        "print(trigram_table.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJLJ2oF9UIWr",
        "outputId": "db6fbc98-325b-4bc1-c201-448c6fbf2e7b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Model\n",
            "         Word  Count  Probability\n",
            "0         <s>      1     0.017544\n",
            "1     natural      1     0.017544\n",
            "2    language      4     0.070175\n",
            "3  processing      1     0.017544\n",
            "4       field      1     0.017544\n",
            "\n",
            "Bigram Model\n",
            "                   Bigram  Count  Conditional Probability\n",
            "0          (<s>, natural)      1                     1.00\n",
            "1     (natural, language)      1                     1.00\n",
            "2  (language, processing)      1                     0.25\n",
            "3     (processing, field)      1                     1.00\n",
            "4     (field, artificial)      1                     1.00\n",
            "\n",
            "Trigram Model\n",
            "                             Trigram  Count  Conditional Probability\n",
            "0           (<s>, natural, language)      1                      1.0\n",
            "1    (natural, language, processing)      1                      1.0\n",
            "2      (language, processing, field)      1                      1.0\n",
            "3    (processing, field, artificial)      1                      1.0\n",
            "4  (field, artificial, intelligence)      1                      1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 6**\n",
        "\n",
        "Apply Smoothing\n",
        "\n",
        "• Add-one (Laplace) smoothing\n",
        "\n",
        "Explain in 3–4 lines:\n",
        "Why smoothing is needed and what problem it solves."
      ],
      "metadata": {
        "id": "gJabyU90UyOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary size\n",
        "vocab_size = len(unigram_counts)\n",
        "\n",
        "\n",
        "# ---------- SMOOTHED BIGRAM PROBABILITY ----------\n",
        "\n",
        "def laplace_bigram_probability(w1, w2):\n",
        "    \"\"\"\n",
        "    P(w2 | w1) with Add-One smoothing\n",
        "    \"\"\"\n",
        "    bigram = (w1, w2)\n",
        "    return (bigram_counts.get(bigram, 0) + 1) / (unigram_counts.get(w1, 0) + vocab_size)\n",
        "\n",
        "\n",
        "# Example: smoothed bigram probabilities\n",
        "print(\"Smoothed Bigram Probability Example:\")\n",
        "print(\"P(processing | language) =\",\n",
        "      laplace_bigram_probability('language', 'processing'))\n",
        "\n",
        "\n",
        "# ---------- SMOOTHED TRIGRAM PROBABILITY ----------\n",
        "\n",
        "def laplace_trigram_probability(w1, w2, w3):\n",
        "    \"\"\"\n",
        "    P(w3 | w1, w2) with Add-One smoothing\n",
        "    \"\"\"\n",
        "    trigram = (w1, w2, w3)\n",
        "    bigram = (w1, w2)\n",
        "    return (trigram_counts.get(trigram, 0) + 1) / (bigram_counts.get(bigram, 0) + vocab_size)\n",
        "\n",
        "\n",
        "# Example: smoothed trigram probabilities\n",
        "print(\"\\nSmoothed Trigram Probability Example:\")\n",
        "print(\"P(is | language, processing) =\",\n",
        "      laplace_trigram_probability('language', 'processing', 'is'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9KNtgVqUPc7",
        "outputId": "f20aef65-20e2-41f2-f43c-d5380320cb6f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smoothed Bigram Probability Example:\n",
            "P(processing | language) = 0.04\n",
            "\n",
            "Smoothed Trigram Probability Example:\n",
            "P(is | language, processing) = 0.02127659574468085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 7**\n",
        "\n",
        "Sentence Probability Calculation\n",
        "\n",
        "\n",
        "• choose at least 5 sentences\n",
        "\n",
        "• compute probability using:\n",
        "\n",
        "o Unigram model\n",
        "\n",
        "o Bigram model\n",
        "\n",
        "o Trigram model\n",
        "\n"
      ],
      "metadata": {
        "id": "z8PAHpgnWFR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7 — Sentence Probability Calculation\n",
        "\n",
        "import math\n",
        "\n",
        "# Function to preprocess a sentence (same steps as before, but simpler)\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r'[^a-z\\s]', '', sentence)\n",
        "    words = word_tokenize(sentence)\n",
        "    words = ['<s>'] + words + ['</s>']\n",
        "    return words\n",
        "\n",
        "\n",
        "# ---------- UNIGRAM SENTENCE PROBABILITY ----------\n",
        "\n",
        "def sentence_probability_unigram(sentence):\n",
        "    words = preprocess_sentence(sentence)\n",
        "    prob = 1.0\n",
        "\n",
        "    for word in words:\n",
        "        prob *= unigram_prob.get(word, 1 / len(unigram_counts))  # small fallback\n",
        "\n",
        "    return prob\n",
        "\n",
        "\n",
        "# ---------- BIGRAM SENTENCE PROBABILITY (Laplace Smoothed) ----------\n",
        "\n",
        "def sentence_probability_bigram(sentence):\n",
        "    words = preprocess_sentence(sentence)\n",
        "    prob = 1.0\n",
        "\n",
        "    for i in range(len(words) - 1):\n",
        "        prob *= laplace_bigram_probability(words[i], words[i+1])\n",
        "\n",
        "    return prob\n",
        "\n",
        "\n",
        "# ---------- TRIGRAM SENTENCE PROBABILITY (Laplace Smoothed) ----------\n",
        "\n",
        "def sentence_probability_trigram(sentence):\n",
        "    words = preprocess_sentence(sentence)\n",
        "    prob = 1.0\n",
        "\n",
        "    for i in range(len(words) - 2):\n",
        "        prob *= laplace_trigram_probability(words[i], words[i+1], words[i+2])\n",
        "\n",
        "    return prob\n",
        "\n",
        "\n",
        "# Choose at least 5 sentences\n",
        "sentences = [\n",
        "    \"Natural language processing is important\",\n",
        "    \"Language models predict words\",\n",
        "    \"NLP is used in chatbots\",\n",
        "    \"Students learn artificial intelligence\",\n",
        "    \"Computers understand human language\"\n",
        "]\n",
        "\n",
        "\n",
        "# Compute probabilities\n",
        "for sentence in sentences:\n",
        "    print(\"\\nSentence:\", sentence)\n",
        "    print(\"Unigram Probability:\", sentence_probability_unigram(sentence))\n",
        "    print(\"Bigram Probability:\", sentence_probability_bigram(sentence))\n",
        "    print(\"Trigram Probability:\", sentence_probability_trigram(sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuiVoWT3Xhh7",
        "outputId": "580170c7-93cb-4fc1-f8ac-a8c8be84513c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence: Natural language processing is important\n",
            "Unigram Probability: 5.070876356831149e-12\n",
            "Bigram Probability: 6.979548666089596e-10\n",
            "Trigram Probability: 1.8207518259364167e-08\n",
            "\n",
            "Sentence: Language models predict words\n",
            "Unigram Probability: 2.8903995233937555e-10\n",
            "Bigram Probability: 1.6401939365310554e-08\n",
            "Trigram Probability: 4.2787667909505786e-07\n",
            "\n",
            "Sentence: NLP is used in chatbots\n",
            "Unigram Probability: 6.283477224769034e-12\n",
            "Bigram Probability: 8.914097481147038e-11\n",
            "Trigram Probability: 4.855241555647359e-09\n",
            "\n",
            "Sentence: Students learn artificial intelligence\n",
            "Unigram Probability: 3.6129994042421943e-11\n",
            "Bigram Probability: 8.910062126922891e-09\n",
            "Trigram Probability: 2.1858917301595349e-07\n",
            "\n",
            "Sentence: Computers understand human language\n",
            "Unigram Probability: 1.1663015620711644e-10\n",
            "Bigram Probability: 3.278902862707624e-08\n",
            "Trigram Probability: 8.375458399307515e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The unigram model calculates probability based only on individual word frequencies.\n",
        "The bigram model considers the probability of a word given the previous word.\n",
        "The trigram model uses two previous words for prediction, making it more context-aware.\n",
        "Generally, longer sentences or unseen word combinations result in lower probabilities.\n",
        "A lower probability means the sentence is less likely according to the trained language model."
      ],
      "metadata": {
        "id": "64afMyMcXyqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 8**\n",
        "\n",
        "Perplexity Calculation\n",
        "\n",
        "• compute perplexity for test sentences\n",
        "\n",
        "• compare perplexity across models\n"
      ],
      "metadata": {
        "id": "JKKNpOg3X2_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8 — Perplexity Calculation\n",
        "\n",
        "import math\n",
        "\n",
        "# Function to compute perplexity\n",
        "def calculate_perplexity(sentence, model_type=\"unigram\"):\n",
        "\n",
        "    words = preprocess_sentence(sentence)\n",
        "    N = len(words)\n",
        "\n",
        "    if model_type == \"unigram\":\n",
        "        prob = sentence_probability_unigram(sentence)\n",
        "\n",
        "    elif model_type == \"bigram\":\n",
        "        prob = sentence_probability_bigram(sentence)\n",
        "\n",
        "    elif model_type == \"trigram\":\n",
        "        prob = sentence_probability_trigram(sentence)\n",
        "\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "    # Avoid log(0)\n",
        "    if prob == 0:\n",
        "        return float(\"inf\")\n",
        "\n",
        "    # Perplexity formula\n",
        "    perplexity = pow(prob, -1/N)\n",
        "    return perplexity\n",
        "\n",
        "\n",
        "# Test sentences (at least 5)\n",
        "test_sentences = [\n",
        "    \"Natural language processing is important\",\n",
        "    \"Language models predict words\",\n",
        "    \"NLP is used in chatbots\",\n",
        "    \"Students learn artificial intelligence\",\n",
        "    \"Computers understand human language\"\n",
        "]\n",
        "\n",
        "\n",
        "# Compute perplexity for each model\n",
        "for sentence in test_sentences:\n",
        "    print(\"\\nSentence:\", sentence)\n",
        "\n",
        "    uni_pp = calculate_perplexity(sentence, \"unigram\")\n",
        "    bi_pp = calculate_perplexity(sentence, \"bigram\")\n",
        "    tri_pp = calculate_perplexity(sentence, \"trigram\")\n",
        "\n",
        "    print(\"Unigram Perplexity:\", uni_pp)\n",
        "    print(\"Bigram Perplexity:\", bi_pp)\n",
        "    print(\"Trigram Perplexity:\", tri_pp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPoRDEuFYOX7",
        "outputId": "5060b302-af1d-41f7-ed1d-41f9ed737543"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence: Natural language processing is important\n",
            "Unigram Perplexity: 41.0732972673736\n",
            "Bigram Perplexity: 20.32472441878892\n",
            "Trigram Perplexity: 12.754941283097695\n",
            "\n",
            "Sentence: Language models predict words\n",
            "Unigram Perplexity: 38.890215872352385\n",
            "Bigram Perplexity: 19.83889442691526\n",
            "Trigram Perplexity: 11.51985193867672\n",
            "\n",
            "Sentence: NLP is used in chatbots\n",
            "Unigram Perplexity: 39.83429510112762\n",
            "Bigram Perplexity: 27.27113614925815\n",
            "Trigram Perplexity: 15.405796807050338\n",
            "\n",
            "Sentence: Students learn artificial intelligence\n",
            "Unigram Perplexity: 54.99907073029815\n",
            "Bigram Perplexity: 21.962741228969545\n",
            "Trigram Perplexity: 12.884331557608252\n",
            "\n",
            "Sentence: Computers understand human language\n",
            "Unigram Perplexity: 45.24092998109368\n",
            "Bigram Perplexity: 17.675779484371482\n",
            "Trigram Perplexity: 10.29987377270416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 9**\n",
        "\n",
        "Comparison and Analysis\n",
        "\n",
        "• Which model gave lowest perplexity?\n",
        "\n",
        "• Did trigrams always perform best?\n",
        "\n",
        "• What happens when unseen words appear?\n",
        "\n",
        "• How did smoothing affect results?\n",
        "\n",
        "Write comparison in 8–10 sentences."
      ],
      "metadata": {
        "id": "KJNWmOEnYfUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our experiments, the trigram model generally produced the lowest perplexity, indicating that it was better at predicting the test sentences compared to the unigram and bigram models. This is because trigrams use more contextual information (two previous words), allowing more accurate probability estimation. The bigram model performed better than the unigram model in most cases, since it considers word-to-word relationships instead of treating words independently. However, trigrams did not always perform best for every sentence, especially when the training data was small or when certain three-word combinations were rare.\n",
        "\n",
        "When unseen words or unseen N-grams appeared in the test sentences, the probability became very small or zero without smoothing. This caused perplexity to increase significantly. The zero-probability problem is common in language models when encountering new word combinations not present in the training corpus.\n",
        "\n",
        "Applying Add-One (Laplace) smoothing solved this issue by assigning a small non-zero probability to unseen N-grams. As a result, the model became more robust and avoided infinite perplexity values. Although smoothing slightly lowers the probability of frequent N-grams, it improves overall model stability. Therefore, smoothing plays an essential role in making N-gram language models practical and reliable."
      ],
      "metadata": {
        "id": "BpcejglnZBi7"
      }
    }
  ]
}